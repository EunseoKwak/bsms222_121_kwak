---
title: "R Notebook"
output: html_notebook
---
# Chapter 15. Statistical inference

## 15.1 Polls

Opinion polling; goal is to describe the opinions held by a specific population on a given topic.

Polls are useful when interviewing every member of a particular population is logistically impossible.

You interview a smaller group chosen randomly and infer the opinions of the general population: _Inference_

Elections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. 

Similar polls are conducted by news organizations because results tend to be of interest to the general public and made public.

Poll results reporting estimates of the popular vote for the 2016 presidential election:

First, note that different polls, report a different spread.
Notice also that the reported spreads hover around what ended up being the actual result.
We also see a column titled MoE which stands for margin of error.

### 15.1.1 The sampling model for polls

We will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this urn (in this case, a pickle jar)

Before making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you $0.10 per each bead you sample.

Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.

```{r}
library(tidyverse)
library(dslabs)
take_poll(25)
```

The beads inside the urn represent the individuals that will vote on election day.

## 15.2 Populations, samples, parameters, and estimates

Proportion of blue beads: p, red beadsL 1-p, and the spread p-(1-p)=2p-1

The beads in the urn are called the population. The proportion of blue beads in the population p is called a parameter. The 25 beads we see in the previous plot are called a sample. 

If we run the command take_poll(25) four times, we get a different answer each time, since the sample proportion is a random variable.

### 15.2.1 The sample average

We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter p.

Define Random variable X as X=1 for blue, X=0 for red.
The population is a list of 0s and 1s.

Sample N beads; average of the draws X1, ..., XN is equivalent to the proportion of blue beads in our sample.

In general, in statistics textbooks a bar on top of a symbol means the average.

Xbar=1/N x i가 1부터 N까지 시그마 Xi

Inependent: the expected value of the sum of draws is N times the average of the values in the urn.

Here we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to estimate $p$.

### 15.2.2 Parameters

We define parameters to define unknown parts of our models.

Our main goal is figuring out what is p, so we are going to estimate this parameter.

### 15.2.3 Polling versus forecasting

If a poll is conducted four months before the election, it is estimating the p for that moment and not for election day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. 

### 15.2. Properties of our estimate: expected value and standard error

Using what we have learned, the expected value of the sum NXbar is Nx the average of the urn, p. So dividing by the non-random constant N gives us that the expected value of the average Xbar is p.

$$
E(\bar{X})= p
$$
$$
SE(\bar{X})=\sqrt{p(1-p)/N}
$$
The expected value of the sample proportion Xbar is the parameter of interest p and we can make the standard error as small as we want by increasing N. The law of large numbers tells us that with a large enough poll, our estimate converges to p.

But how large? Also, one problem is that we do not know p, so we can’t compute the standard error.

For a sample size of 1,000 and p=0.51, the standard error is:

```{r}
p=0.51
sqrt(p*(1-p))/sqrt(1000)
```

So even with large polls, for close elections, Xbar can lead us astray if we don’t realize it is a random variable. 

## 15.3 Exercises

1. Suppose you poll a population in which a proportion p of voters are Democrats and 1-p are Republicans. Your sample size is N=25. Consider the random variable S which is the total number of Democrats in your sample. What is the expected value of this random variable? Hint: it’s a function of  
p.

__25p__

2. What is the standard error of S? Hint: it’s a function of p.

__$\sqrt{25p(1-p)}$__

3. Consider the random variable S/N. This is equivalent to the sample average, which we have been denoting as $\bar{X}$. What is the expected value of the  $\bar{X}$? Hint: it’s a function of p.

__$\sqrt{p(1-p)/N}$__

4. What is the standard error of $\bar{X}$? Hint: it’s a function of p.

__$\sqrt{p(1-p)/N}$__

5. Write a line of code that gives you the standard error se for the problem above for several values of p, specifically for p <- seq(0, 1, length = 100). Make a plot of se versus p.

```{r}
N <- 25
p <- seq(0, 1, length = 100)
se <- sqrt(p*(1-p)/N)
plot(p, se)
```

6. Copy the code above and put it inside a for-loop to make the plot for N=25, N=100, and N=1000.

```{r}
N <- c(25, 100, 1000)
for(n in N){se <- sqrt(p*(1-p)/n)
plot(p, se)
}
```

7. If we are interested in the difference in proportions p-(1-p), our estimate is $d = \bar{X} - (1-\bar{X})$. Use the rules we learned about sums of random variables and scaled random variables to derive the expected value of d.

__$E[2\bar{X}-1] = 2E[\bar{X}]-1 = 2p-1$__

8. What is the standard error of d?

__$SE[2\bar{X}-1] = 2SE[\bar{X}] = 2\sqrt{p(1-p)/N}$__

9. If the actual p=.45, it means the Republicans are winning by a relatively large margin since d=−.1 , which is a 10% margin of victory. In this case, what is the standard error of $2\hat{X}-1$ if we take a sample of N=25?

```{r}
N <- 25
p <- 0.45
2*sqrt(p*(1-p)/N)
```

10. Given the answer to 9, which of the following best describes your strategy of using a sample size of N=25?

A. The expected value of our estimate $2\bar{X}-1$ is d, so our prediction will be right on.
__B. Our standard error is larger than the difference, so the chances of $2\bar{X}-1$ being positive and throwing us off were not that small. We should pick a larger sample size.__
C. The difference is 10% and the standard error is about 0.2, therefore much smaller than the difference.
D. Because we don't know p, we have no way of knowing that making N larger would actually improve our standard error.

## 15.4 Central limit theorem in practice

The CLT tells us that the distribution function for a sum of draws is approximately normal. Dividing a normally distributed random variable by a constant is also a normally distributed variable. The distribution of $\bar{X}$ is approximately normal.

$\bar{X}$ has an approximately normal distribution with expected value p and standard error $\sqrt{p(1-p)/N}$

Subtract the expected value and divide by the standard error to get a standard normal random variable, call it Z.

One problem we have is that since we don’t know  
p, we don’t know $SE(\bar{X})$.

But it turns out that the CLT still works if we estimate the standard error by using $\bar{X}$ in place of p.

```{r}
x_hat <- 0.48
se <- sqrt(x_hat*(1-x_hat)/25)
se
```

```{r}
pnorm(0.01/se) - pnorm(-0.01/se)
```

Earlier we mentioned the margin of error. Now we can define it because it is simply two times the standard error, which we can now estimate.

```{r}
1.96*se
```

Why do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from p, we get which we know is about 95%:

```{r}
pnorm(1.96)-pnorm(-1.96)
```

there is a 95% probability that $\bar{X}$ will be within 1.96 × $\hat{SE}(\bar{X})$.

We often round 1.96 up to 2 for simplicity of presentation.

In summary, the CLT tells us that our poll based on a sample size of 25 is not very useful. We don’t really learn much when the margin of error is this large.

### 15.4.1 A monte Carlo simulation

```{r}
B <- 10000
N <- 1000
x_hat <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})
```

Problem; we don't know p.
We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function take_poll(n=1000) instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.

One thing we therefore do to corroborate theoretical results is to pick one or several values of p and run the simulations. Let’s set p=0.45. We can then simulate a poll:

```{r}
p <- 0.45
N <- 1000

x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat <- mean(x)
```

Our estimate: x_hat
we can use that code to do a monte carlo simulation

```{r}
B <- 10000
x_hat <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})
```

```{r}
mean(x_hat)
sd(x_hat)
```

### 15.4.2 The spread

The competition is to predict the spread, not the proportion p.
two parties; spread is 2p-1.

We have our estimate $\bar{X}$ and $\hat{SE(\bar{X})}$ , we estimate the spread with  $2\bar{X}$-1 and, since we are multiplying by 2, the standard error is 2$\hat{SE(\bar{X})}$.

### 15.4.3 Bias: Why not run a very large poll?

Running a large poll is very expensive.
Theory has its limitations - polling is much more complicated.
We don't actually know for sure who is in our population and who is not.
Hence, even if our margin of error is very small, it might not be exactly right that our expected value is p : bias.
typical bias: 1-2%.

## 15.5 Exercises

1. Write an urn model function that takes the proportion of Democrats p, iand the sample size N as arguments and returns the sample average if Democrats are 1s and Republicans are 0s. Call the function take_sample.

```{r}
take_sample <- function(p, N){x<- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p,p))
mean(x)}
```

2. Now assume p <- 0.45 and that your sample size is N=100. Take a sample 10,000 times and save the vector of mean(X) - p into an object called errors. Hint: use the function you wrote for exercise 1 to write this in one line of code.

```{r}
p <- 0.45
N <- 100
B <- 10000
errors <- replicate(B, take_sample(p, N)-p)
```

3. The vector errors contains, for each simulated sample, the difference between the actual p and our estimate $\bar{X}$. We refer to this difference as the error. Compute the average and make a histogram of the errors generated in the Monte Carlo simulation and select which of the following best describes their distributions:

```{r}
mean(errors)
hist(errors)
```

A. The errors are all about 0.05.
B. The errors are all about -0.05.
__C. The errors are symmetrically distributed around 0.__
D. The errors range from -1 to 1.

4.  The error $\bar{X}-p$ is a random variable. In practice, the error is not observed because we do not know p. Here we observe it because we constructed the simulation. What is the average size of the error if we define the size by taking the absolute value $\mid \bar{X}-p\mid$?

```{r}
mean(abs(errors))
```

------------------------- 개념 헷갈림
5. The standard error is related to the typical size of the error we make when predicting. We say size because we just saw that the errors are centered around 0, so thus the average error value is 0. For mathematical reasons related to the Central Limit Theorem, we actually use the standard deviation of errors rather than the average of the absolute values to quantify the typical size. What is this standard deviation of the errors?
----------------------------------

6. The theory we just learned tells us what this standard deviation is going to be because it is the standard error of $\bar{X}$. What does theory tell us is the standard error of $\bar{X}$ for a sample size of 100?

```{r}
N <- 100
sqrt(p*(1-p)/N)
```

7. In practice, we don’t know p, so we construct an estimate of the theoretical prediction based by plugging in $\bar{X}$ for p. Compute this estimate. Set the seed at 1 with set.seed(1).

```{r}
set.seed(1)
X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1 - p, p))
barX <- mean(X)
sqrt(barX*(1-barX)/N)
```
-------------------------
8. Note how close the standard error estimates obtained from the Monte Carlo simulation (exercise 5), the theoretical prediction (exercise 6), and the estimate of the theoretical prediction (exercise 7) are. The theory is working and it gives us a practical approach to knowing the typical error we will make if we predict p with $\bar{X}$. Another advantage that the theoretical result provides is that it gives an idea of how large a sample size is required to obtain the precision we need. Earlier we learned that the largest standard errors occur for p=0.5. Create a plot of the largest standard error for N ranging from 100 to 5,000. Based on this plot, how large does the sample size have to be to have a standard error of about 1%?

A. 100
B. 500
C. 2,500
D. 4,000
---------------------

9. For sample size $N=100$, the central limit theorem tells us that the distribution of $\bar{X}$ is:

A. practically equal to $p$.  
__B. approximately normal with expected value $p$ and standard error $\sqrt{p(1-p)/N}$.__
C. approximately normal with expected value $\bar{X}$ and standard error $\sqrt{\bar{X}(1-\bar{X})/N}$.  
D. not a random variable.  

10. Based on the answer from exercise 8, the error $\bar{X} - p$ is:

A. practically equal to 0.  
__B. approximately normal with expected value $0$ and standard error $\sqrt{p(1-p)/N}$.__
C. approximately normal with expected value $p$ and standard error $\sqrt{p(1-p)/N}$.  
D. not a random variable.  

11. To corroborate your answer to exercise 9, make a qq-plot of the errors you generated in exercise 2 to see if they follow a normal distribution.

```{r}
qqnorm(errors)
qqline(errors)
```

12. If p=0.45 and N=100 as in exercise 2, use the CLT to estimate the probability that $\bar{X}$>0.5. You can assume you know p=0.45 for this calculation.

```{r}
p <- 0.45
N <- 100
1-pnorm(0.5, p, sqrt(p*(1-p)/N))
```

13. Assume you are in a practical situation and you don’t know p. Take a sample of size N=100 and obtain a sample average of $\bar{X}$=0.51. What is the CLT approximation for the probability that your error is equal to or larger than 0.01?

```{r}
Xbar <- 0.51
se <- sqrt(Xbar*(1-Xbar)/N)
1 - pnorm(0.01, 0, se) + pnorm(-0.01, 0, se)
```

## 15.6 Confidence intervals

Confidence intervals are a very useful concept widely employed by data analysts. A version of these that are commonly seen come from the ggplot geometry geom_smooth.

Shaded area around the curve: created using the concept of confidence intervals.

We need to be balanced between having small interval and including p in the interval.

We want to know the probability that the interval [$\bar{X}$-2$\hat{SE(\bar{X})}$, $\bar{X}$+2$\hat{SE(\bar{X})}$] contains the true proportion p.

start and end of this intervals are random variables

```{r}
p <- 0.45
N <- 1000
```

```{r}
x <- sample(c(0, 1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat <- mean(x)
se_hat <- sqrt(x_hat * (1 - x_hat) / N)
c(x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)
```

is different from this one

```{r}
x <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
x_hat <- mean(x)
se_hat <- sqrt(x_hat * (1 - x_hat) / N)
c(x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)
```

-> Random variation

so we have

$$
Pr(-1.96 <= Z <= 1.96)
$$
which can be computed

```{r}
pnorm(1.96)-pnorm(-1.96)
```

we have a 95% probability.

For 99% probability:

```{r}
z <- qnorm(0.995)
z
```

```{r}
pnorm(z) - pnorm(-z)
```

we set z=qnorm(1-(1-p)/2) because 1-(1-p)/2+(1-p)/2=p

```{r}
qnorm(0.975)
```

### 15.6.1 A Monte Carlo Simulation

We can run a monte carlo simulation to confirm that a 95% confidence interval includes p 95% of the time

```{r}
N <- 1000
B <- 10000
inside <- replicate(B, {
  x <- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  x_hat <- mean(x)
  se_hat <- sqrt(x_hat * (1 - x_hat) / N)
  between(p, x_hat - 1.96 * se_hat, x_hat + 1.96 * se_hat)
})
mean(inside)
```

### 15.6.2 The correct language

Remember that it is the intervals that are random, not p.

## 15.7 Exercises

```{r}
library(dslabs)
data("polls_us_election_2016")
```

```{r}
library(tidyverse)
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.") 
```

1. For the first poll, you can obtain the samples size and estimated Clinton percentage with:

```{r}
N <- polls$samplesize[1]
x_hat <- polls$rawpoll_clinton[1]/100
```

Assume there are only two candidates and construct a 95% confidence interval for the election night proportion p.

```{r}
se_hat <- sqrt(x_hat * (1-x_hat) / N)
c(x_hat-1.96 * se_hat, x_hat+1.96 * se_hat)
```

2. Now use dplyr to add a confidence interval as two columns, call them lower and upper, to the object poll. Then use select to show the pollster, enddate, x_hat,lower, upper variables. Hint: define temporary columns x_hat and se_hat.

```{r}
polls %>% mutate(x_hat = polls$rawpoll_clinton/100, se_hat = sqrt(x_hat*(1-x_hat)/samplesize),
                 lower = x_hat - 1.96*se_hat, upper = x_hat + 1.96*se_hat) %>%
  select(pollster, enddate, x_hat, lower, upper)
```

3. The final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column, call it hit, to the previous table stating if the confidence interval included the true proportion p=0.482 or not. 

```{r}
polls %>% mutate(x_hat = polls$rawpoll_clinton/100, se_hat = sqrt(x_hat*(1-x_hat)/samplesize),
                 lower = x_hat - 1.96*se_hat, upper = x_hat + 1.96*se_hat) %>%
  select(pollster, enddate, x_hat, lower, upper) %>% mutate(hit=lower<=0.482 & upper>=0.482) 
```

4. For the table you just created, what proportion of confidence intervals included p?

```{r}
polls %>% mutate(x_hat = polls$rawpoll_clinton/100, se_hat = sqrt(x_hat*(1-x_hat)/N),
                 lower = x_hat - 1.96*se_hat, upper = x_hat + 1.96*se_hat) %>%
  select(pollster, enddate, x_hat, lower, upper) %>% mutate(hit=lower<=0.482 & upper>=0.482) %>% summarize(mean(hit))
```

5. If these confidence intervals are constructed correctly, and the theory holds up, what proportion should include p?

__0.95__

6. A much smaller proportion of the polls than expected produce confidence intervals containing p. If you look closely at the table, you will see that most polls that fail to include p are underestimating. The reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. Because, historically, undecideds divide evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates d, which in this election was 0.482−0.461=0.021. Assume that there are only two parties and that d=2p−1, redefine polls as below and re-do exercise 1, but for the difference.

```{r}
polls <- polls_us_election_2016 %>% 
  filter(enddate >= "2016-10-31" & state == "U.S.")  %>%
  mutate(d_hat = rawpoll_clinton / 100 - rawpoll_trump / 100)
```

```{r}
N <- polls$samplesize[1]
d_hat <- polls$d_hat[1]
x_hat <- (d_hat+1)/2
se_hat <- 2*sqrt(x_hat*(1-x_hat)/N)
c(d_hat-1.96 * se_hat, d_hat+1.96 * se_hat)
```

7. Now repeat exercise 3, but for the difference.

```{r}
polls %>% mutate(X_hat = (d_hat+1)/2, se_hat = 2*sqrt(X_hat*(1-X_hat)/samplesize), lower = d_hat-1.96*se_hat, upper = d_hat + 1.96*se_hat) %>% select(pollster, enddate, d_hat, lower, upper) %>% mutate(hit = lower<=0.021 & upper>=0.021)
```

8. Now repeat exercise 4, but for the difference.

```{r}
polls %>% mutate(X_hat = (d_hat+1)/2, se_hat = 2*sqrt(X_hat*(1-X_hat)/samplesize), lower = d_hat - 1.96*se_hat, upper = d_hat+1.96*se_hat) %>% select(pollster, enddate, d_hat, lower, upper) %>% mutate(hit = lower<=0.021 & upper>=0.021) %>% summarize(mean(hit))
```

9. Although the proportion of confidence intervals goes up substantially, it is still lower than 0.95. In the next chapter, we learn the reason for this. To motivate this, make a plot of the error, the difference between each poll’s estimate and the actual d=0.021. Stratify by pollster.

```{r}
polls %>% mutate(error = d_hat - 0.021) %>% 
  ggplot(aes(pollster, error)) +
  geom_point()
```

10. Redo the plot that you made for exercise 9, but only for pollsters that took five or more polls.

```{r}
polls %>% mutate(error = d_hat - 0.021) %>%
  group_by(pollster) %>%
  filter(n() >= 5) %>%
  ggplot(aes(pollster, error)) +
  geom_point()
```

## 15.8 Power

When we took a 25 bead sample size, the confidence interval for the spread:

```{r}
N <- 25
x_hat <- 0.48
(2 * x_hat - 1) + c(-1.96, 1.96) * 2 * sqrt(x_hat * (1 - x_hat) / N)
```

includes 0.

Given the sample size and the value of p, we would have to sacrifice on the probability of an incorrect call to create an interval that does not include 0.

Increasing sample size; lower se and have much better chance of detecting the direction of the spread

## 15.9 P-values

I want to know if the spread 2p-1>0.
N=100, we observe 52 blue beads, 2Xbar-1=0.04.

the p-value is the answer to the question: how likely is it to see a value this large, when the null hypothesis is true? 

```{r}
N <- 100
z <- sqrt(N)*0.02/0.5
1 - (pnorm(z) - pnorm(-z))
```

there is actually a large chance of seeing 52 or larger under the null hypothesis.

If a 95% confidence interval of the spread does not include 0, we know that the p-value must be smaller than 0.05.

## 15.10 Association tests

```{r}
library(tidyverse)
library(dslabs)
data("research_funding_rates")
research_funding_rates %>% select(discipline, applications_total, 
                                  success_rates_total) %>% head()
```

```{r}
names(research_funding_rates)
```

```{r}
totals <- research_funding_rates %>% 
  select(-discipline) %>% 
  summarize_all(sum) %>%
  summarize(yes_men = awards_men, 
            no_men = applications_men - awards_men, 
            yes_women = awards_women, 
            no_women = applications_women - awards_women) 
```

larger percent of men than woman received awards:
```{r}
totals %>% summarize(percent_men = yes_men/(yes_men+no_men),
                     percent_women = yes_women/(yes_women+no_women))
```

## 15.10.1 Lady Tasting tea

An acquaintance of Fisher’s claimed that she could tell if milk was added before or after tea was poured.
The null hypothesis here is that she is guessing.
Fisher derived the distribution for the number of correct picks on the assumption that the choices were random and independent.

The basic question we ask is: if the tester is actually guessing, what are the chances that she gets 3 or more correct?

The chance of observing a 3 or something more extreme, under the null hypothesis, is ≈0.24.

## 15.10.2 Two-by-two tables

```{r}
tab <- matrix(c(3,1,1,3),2,2)
rownames(tab)<-c("Poured Before","Poured After")
colnames(tab)<-c("Guessed before","Guessed after")
tab
```

function 'fisher.test' : inference calculation above:

```{r}
fisher.test(tab, alternative="greater")$p.value
```

### 15.10.3 Chi-square test

```{r}
totals %>% summarize(percent_men = yes_men/(yes_men+no_men),
                     percent_women = yes_women/(yes_women+no_women))
```

```{r}
rate <- totals %>%
  summarize(percent_total = 
              (yes_men + yes_women)/
              (yes_men + no_men +yes_women + no_women)) %>%
  pull(percent_total)
rate
```

create 2-by-2 table

```{r}
two_by_two <- data.frame(awarded = c("no", "yes"), 
                     men = c(totals$no_men, totals$yes_men),
                     women = c(totals$no_women, totals$yes_women))
two_by_two
```

compare table to what you expect to see

```{r}
data.frame(awarded = c("no", "yes"), 
       men = (totals$no_men + totals$yes_men) * c(1 - rate, rate),
       women = (totals$no_women + totals$yes_women) * c(1 - rate, rate))
```

We can see that more men than expected and fewer women than expected received funding. 

under null hypothesis; random variable.s

The Chi-square test tells us how likely it is to see a deviation this large or larger. This test uses an asymptotic result, similar to the CLT, related to the sums of independent binary outcomes. The R function chisq.test takes a two-by-two table and returns the results from the test:

```{r}
chisq_test <- two_by_two %>% select(-awarded) %>% chisq.test()
chisq_test$p.value
```

## 15.10.4 The odds ratio

The odds of getting funded if you are a man is:

```{r}
odds_men <- with(two_by_two, (men[2]/sum(men)) / (men[1]/sum(men)))
odds_men
```

And the odds of being funded if you are a woman is:

```{r}
odds_women <- with(two_by_two, (women[2]/sum(women)) / (women[1]/sum(women)))
odds_women
```

### 15.10.5 Confidence intervals for the odds ratio

Not mathematically straightforward.

Confidence interval:

```{r}
log_or <- log(odds_men / odds_women)
se <- two_by_two %>% select(-awarded) %>%
  summarize(se = sqrt(sum(1/men) + sum(1/women))) %>%
  pull(se)
ci <- log_or + c(-1,1) * qnorm(0.975) * se
```

back to odds ratio scale, exponentiate:

```{r}
exp(ci)
```

1 is not included; p-value<0.05

```{r}
2*(1 - pnorm(log_or, 0, se))
```

This is a slightly different p-value than that with the Chi-square test. This is because we are using a different asymptotic approximation to the null distribution.

### 15.10.6 Small count correction

Note that the log odds ratio is not defined if any of the cells of the two-by-two table is 0. For this situation, it is common practice to avoid 0s by adding 0.5 to each cell. Haldane-Anscombe correction.

### 15.10.7 Large samples, small p-values

Reporting only p-values is not an appropriate way to report the results of data analysis.

Note that the relationship between odds ratio and p-value is not one-to-one. It depends on the sample size. So a very small p-value does not necessarily mean a very large odds ratio. 

multiply table by 10, doesn't change odds ratio:

```{r}
two_by_two %>% select(-awarded) %>%
  mutate(men = men*10, women = women*10) %>%
  chisq.test() %>% .$p.value
```

## 15.11 Exercises

1. A famous athlete has an impressive career, winning 70% of her 500 career matches. However, this athlete gets criticized because in important events, such as the Olympics, she has a losing record of 8 wins and 9 losses. Perform a Chi-square test to determine if this losing record can be simply due to chance as opposed to not performing well under pressure.

```{r}
two_by_two <- data.frame(results = c("wins", "losses"), 
                     important_events = c(8, 9),
                     not_important_events = c(350-8, 150-9))
two_by_two
chisq_test <- two_by_two %>% select(-results) %>% chisq.test()
chisq_test$p.value
```

2. Why did we use the Chi-square test instead of Fisher’s exact test in the previous exercise?

A. It actually does not matter, since they give the exact same p-value.
B. Fisher’s exact and the Chi-square are different names for the same test.
__C. Because the sum of the rows and columns of the two-by-two table are not fixed so      the hypergeometric distribution is not an appropriate assumption for the null         hypothesis. For this reason, Fisher’s exact test is rarely applicable with observational data.__
D. Because the Chi-square test runs faster.!

3. Compute the odds ratio of “losing under pressure” along with a confidence interval.

```{r}
odds_important_events <- with(two_by_two, (important_events[1]/sum(important_events)) / (important_events[2]/sum(important_events)))
odds_not_important_events <- with(two_by_two, (not_important_events[1]/sum(not_important_events)) / (not_important_events[2]/sum(not_important_events)))
log_or <- log(odds_important_events / odds_not_important_events)
se <- two_by_two %>% select(-results) %>%
  summarize(se = sqrt(sum(1/important_events) + sum(1/not_important_events))) %>%
  pull(se)
ci <- log_or + c(-1,1) * qnorm(0.975) * se
exp(ci)
```

4. Notice that the p-value is larger than 0.05 but the 95% confidence interval does not include 1. What explains this?

A. We made a mistake in our code.
B. These are not t-tests so the connection between p-value and confidence intervals      does not apply.
__C. Different approximations are used for the p-value and the confidence interval         calculation. If we had a larger sample size the match would be better.__
D. We should use the Fisher exact test to get confidence intervals.

5. Multiply the two-by-two table by 2 and see if the p-value and confidence retrieval are a better match.

```{r}
two_by_two %>% select(-results) %>%
  mutate(important_events = important_events*10, not_important_events = not_important_events*10) %>%
  chisq.test() %>% .$p.value
```
